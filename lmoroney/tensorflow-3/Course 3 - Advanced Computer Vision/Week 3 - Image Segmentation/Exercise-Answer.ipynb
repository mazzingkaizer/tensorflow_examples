{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "M2NIST Image Segmentation Exercise.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6x_sGPQocpw"
      },
      "source": [
        "#TensorFlow - Advanced Computer Vision Module 3 Exercise\n",
        "\n",
        "This notebook is the solution to the exercise of Module 3.\n",
        "\n",
        "**Question**\n",
        "\n",
        "Build a model that predicts the segmentation masks (pixel wise label map) of MNIST handwritten digits. \n",
        "The model should be trained on the [M2NIST dataset](https://www.kaggle.com/farhanhubble/multimnistm2nist).\n",
        "\n",
        "M2NIST is multi digit MNIST. Each image has upto 3 digits from MNIST digits and the corresponding labels file has the segmentstion masks.\n",
        "\n",
        "You can train a CNN from scratch for the downsampling path and use FCN-8 upsampling for producing the pixel wise label map."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZ3g9dJxSxmN"
      },
      "source": [
        "##Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aifz2907kxYN"
      },
      "source": [
        "import os, re, time, json\n",
        "import PIL.Image, PIL.ImageFont, PIL.ImageDraw\n",
        "import numpy as np\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow_datasets as tfds\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "\n",
        "print(\"Tensorflow version \" + tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0m9CxxThS1dg"
      },
      "source": [
        "##Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvjweKXoe4mI"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "#There are 11 classes (0 to 9 digits + background class)\n",
        "n_classes = 11\n",
        "\n",
        "colors = [tuple(np.random.randint(256, size=3) / 255.0) for i in range(n_classes)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RYh6cCzXE6R"
      },
      "source": [
        "##Download Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUGGF3wfqYni"
      },
      "source": [
        "[M2NIST](https://www.kaggle.com/farhanhubble/multimnistm2nist) is **multi digit** [MNIST](http://yann.lecun.com/exdb/mnist/). \n",
        "Each image has upto 3 digits from MNIST digits and the corresponding labels file has the segmentstion masks.\n",
        "\n",
        "The dataset is available on [Kaggle](https://www.kaggle.com).\n",
        "Link to the dataset: https://www.kaggle.com/farhanhubble/multimnistm2nist\n",
        "\n",
        "To make it easier for you, we're hosting it on Google Cloud so you can download without kaggle credentials\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROok0i9rMcu0"
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/m2nist.zip \\\n",
        "    -O /tmp/m2nist.zip\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "\n",
        "\n",
        "local_zip = '/tmp/m2nist.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp/training')\n",
        "zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xy17LYR7XJNa"
      },
      "source": [
        "##Load and Pre Process Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXGMrWnkrvpK"
      },
      "source": [
        "This dataset can be easily preprocessed since it is available as **Numpy Array Files (.npy)**\n",
        "\n",
        "1. **tmp/training/combined.npy** has the image files containing the multiple MNIST digits. Each image is of size **64 x 84**.\n",
        "\n",
        "2. **tmp/training/segmented.npy** has corresponding segmentation masks. Each segmentation mask is also of size **64 x 84**.\n",
        "\n",
        "This dataset has **5000** samples. You can make appropriate training and validation splits as required for the problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jy_pw5I2-xLP"
      },
      "source": [
        "#Training, validation and test sizes\n",
        "train_size = 4000\n",
        "val_size = 800\n",
        "test_size = 200\n",
        "\n",
        "'''\n",
        "This function maps image and segmentation masks. Images are normalized so that each pixel is in the range [-1, 1]\n",
        "'''\n",
        "def read_image_and_annotation(image, annotation):\n",
        "  image = tf.cast(image, dtype=tf.float32)\n",
        "  image = tf.reshape(image, (image.shape[0], image.shape[1], 1,))\n",
        "  annotation = tf.cast(annotation, dtype=tf.int32)\n",
        "  image = image / 127.5\n",
        "  image -= 1\n",
        "  return image, annotation\n",
        "\n",
        "'''\n",
        "This function creates training dataset from training splits of images and segmentation masks.\n",
        "'''\n",
        "def get_training_dataset(images, annos):\n",
        "  training_dataset = tf.data.Dataset.from_tensor_slices((images, annos))\n",
        "  training_dataset = training_dataset.map(read_image_and_annotation)\n",
        "\n",
        "  training_dataset = training_dataset.shuffle(512, reshuffle_each_iteration=True)\n",
        "  training_dataset = training_dataset.batch(BATCH_SIZE)\n",
        "  training_dataset = training_dataset.repeat()\n",
        "  training_dataset = training_dataset.prefetch(-1)\n",
        "\n",
        "  return training_dataset\n",
        "\n",
        "\n",
        "'''\n",
        "This function creates validation dataset from validation splits of images and segmentation masks.\n",
        "'''\n",
        "def get_validation_dataset(images, annos):\n",
        "  validation_dataset = tf.data.Dataset.from_tensor_slices((images, annos))\n",
        "  validation_dataset = validation_dataset.map(read_image_and_annotation)\n",
        "  validation_dataset = validation_dataset.batch(BATCH_SIZE)\n",
        "  validation_dataset = validation_dataset.repeat()\n",
        "\n",
        "  return validation_dataset\n",
        "\n",
        "\n",
        "'''\n",
        "This function creates test dataset from test splits of images and segmentation masks.\n",
        "'''\n",
        "def get_test_dataset(images, annos):\n",
        "  test_dataset = tf.data.Dataset.from_tensor_slices((images, annos))\n",
        "  test_dataset = test_dataset.map(read_image_and_annotation)\n",
        "  test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "  return test_dataset\n",
        "\n",
        "'''\n",
        "This function loads the images and segments as numpy arrays from npy files and makes splits for training, validation and test datasets.\n",
        "'''\n",
        "\n",
        "def load_images_and_segments():\n",
        "  #Loads images and segmentation masks.\n",
        "  images = np.load('/tmp/training/combined.npy')\n",
        "  segments = np.load('/tmp/training/segmented.npy')\n",
        "\n",
        "  #Makes training, validation, test splits from loaded images and segmentation masks.\n",
        "  train_images, val_images, train_annos, val_annos = train_test_split(images, segments, test_size=0.2, shuffle=True)\n",
        "  val_images, test_images, val_annos, test_annos = train_test_split(val_images, val_annos, test_size=0.2, shuffle=True)\n",
        "\n",
        "  return (train_images, train_annos), (val_images, val_annos), (test_images, test_annos)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIS70_um_Y7n"
      },
      "source": [
        "#Load Dataset\n",
        "train_slices, val_slices, test_slices = load_images_and_segments()\n",
        "\n",
        "#Create training, validation, test datasets.\n",
        "training_dataset = get_training_dataset(train_slices[0], train_slices[1])\n",
        "validation_dataset = get_validation_dataset(val_slices[0], val_slices[1])\n",
        "test_Dataset = get_test_dataset(test_slices[0], test_slices[1])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d46YCbvPafbp",
        "cellView": "both"
      },
      "source": [
        "#@title Plot Utilities [RUN ME]\n",
        "def fuse_with_pil(images):\n",
        "  widths = (image.shape[1] for image in images)\n",
        "  heights = (image.shape[0] for image in images)\n",
        "  total_width = sum(widths)\n",
        "  max_height = max(heights)\n",
        "\n",
        "  new_im = PIL.Image.new('RGB', (total_width, max_height))\n",
        "\n",
        "  x_offset = 0\n",
        "  for im in images:\n",
        "    pil_image = PIL.Image.fromarray(np.uint8(im))\n",
        "    new_im.paste(pil_image, (x_offset,0))\n",
        "    x_offset += im.shape[1]\n",
        "  \n",
        "  return new_im\n",
        "\n",
        "def give_color_to_annotation(annotation):\n",
        "  seg_img = np.zeros( (annotation.shape[0],annotation.shape[1],3) ).astype('float')\n",
        "  for c in range(n_classes):\n",
        "    segc = (annotation == c) #np.int32(annotation[:, :, c])\n",
        "    seg_img[:,:,0] += segc*( colors[c][0] * 255.0)\n",
        "    seg_img[:,:,1] += segc*( colors[c][1] * 255.0)\n",
        "    seg_img[:,:,2] += segc*( colors[c][2] * 255.0)\n",
        "  return seg_img\n",
        "\n",
        "def show_annotation_and_prediction(image, annotation, prediction, iou_list, dice_score_list):\n",
        "  new_ann = np.argmax(annotation, axis=2)\n",
        "  true_img = give_color_to_annotation(new_ann)\n",
        "  pred_img = give_color_to_annotation(prediction)\n",
        "\n",
        "\n",
        "  image = image + 1\n",
        "  image = image * 127.5\n",
        "  image = np.reshape(image, (image.shape[0], image.shape[1],))\n",
        "  image = np.uint8(image)\n",
        "  images = [image, np.uint8(pred_img), np.uint8(true_img)]\n",
        "\n",
        "  metrics_by_id = [(idx, iou, dice_score) for idx, (iou, dice_score) in enumerate(zip(iou_list, dice_score_list)) if iou > 0.0 and idx < 10]\n",
        "  metrics_by_id.sort(key=lambda tup: tup[1], reverse=True)  # sorts in place\n",
        "\n",
        "  \n",
        "  display_string_list = [\"{}: IOU: {} Dice Score: {}\".format(idx, iou, dice_score) for idx, iou, dice_score in metrics_by_id]\n",
        "  display_string = \"\\n\".join(display_string_list)\n",
        "\n",
        "  plt.figure(figsize=(15, 4))\n",
        "\n",
        "  for idx, im in enumerate(images):\n",
        "    plt.subplot(1, 3, idx+1)\n",
        "    if idx == 1:\n",
        "      plt.xlabel(display_string)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.imshow(im)\n",
        "    # if idx == 1:\n",
        "    #     plt.text(0.5, 4.2, display_string, verticalalignment='center')\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "def show_annotation_and_image(image, annotation):\n",
        "  new_ann = np.argmax(annotation, axis=2)\n",
        "  seg_img = give_color_to_annotation(new_ann)\n",
        "  \n",
        "  image = image + 1\n",
        "  image = image * 127.5\n",
        "  image = np.reshape(image, (image.shape[0], image.shape[1],))\n",
        "\n",
        "  image = np.uint8(image)\n",
        "  images = [image, seg_img]\n",
        "  \n",
        "  images = [image, seg_img]\n",
        "  fused_img = fuse_with_pil(images)\n",
        "  plt.imshow(fused_img)\n",
        "\n",
        "\n",
        "def list_show_annotation(dataset):\n",
        "  ds = dataset.unbatch()\n",
        "  #ds = ds.shuffle(buffer_size=100)\n",
        "\n",
        "  plt.figure(figsize=(20, 15))\n",
        "  plt.title(\"Images And Annotations\")\n",
        "  plt.subplots_adjust(bottom=0.1, top=0.9, hspace=0.05)\n",
        "\n",
        "  for idx, (image, annotation) in enumerate(ds.take(9)):\n",
        "    plt.subplot(5, 5, idx + 1)\n",
        "    plt.yticks([])\n",
        "    plt.xticks([])\n",
        "    show_annotation_and_image(image.numpy(), annotation.numpy())\n",
        "\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cK621Jm8bJyj"
      },
      "source": [
        "###Let's Take a Look at the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFO_hIhLWYT4"
      },
      "source": [
        "list_show_annotation(training_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdgVkp8wZua0"
      },
      "source": [
        "list_show_annotation(validation_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFv2k8xabRb8"
      },
      "source": [
        "##Define the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_ylpyqJDQiF"
      },
      "source": [
        "Like any FCN, our model will have two paths:\n",
        "1. **Downsampling Path** - Custom CNN from scratch\n",
        "2. **Upsampling Path** - FCN 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHlBUZvsDybt"
      },
      "source": [
        "###Define Baisc Convolution Block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azEEVytHR0Kn"
      },
      "source": [
        "IMAGE_ORDERING = 'channels_last'\n",
        "'''\n",
        "This function defines a basic convolution block having two Conv2D layers each followed by a LeakyReLU and Batch normalization.\n",
        "This is the basic convolution block for our CNN\n",
        "'''\n",
        "def conv_block(x, filters, strides, pooling_size, pool_strides):\n",
        "  x = tf.keras.layers.Conv2D(filters, strides, padding='same', data_format=IMAGE_ORDERING)(x)\n",
        "  x = tf.keras.layers.LeakyReLU()(x)\n",
        "  x = tf.keras.layers.Conv2D(filters, strides, padding='same', data_format=IMAGE_ORDERING)(x)\n",
        "  x = tf.keras.layers.MaxPooling2D(pooling_size, pool_strides, data_format=IMAGE_ORDERING)(x)\n",
        "  x = tf.keras.layers.LeakyReLU()(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "  return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-jJbC91EXTV"
      },
      "source": [
        "###Define DownSampling Path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2VNB99LRwQr"
      },
      "source": [
        "'''\n",
        "This function chains together convolution building blocks to create a feature extraction CNN minus the fully connected layers.\n",
        "'''\n",
        "def FCN8(inputs, nClasses ,  input_height=64, input_width=84):\n",
        "   \n",
        "\n",
        "    img_input = tf.keras.layers.Input(shape=(input_height,input_width, 1)) ## Assume 224,224,3\n",
        "\n",
        "    \n",
        "    ## Block 1\n",
        "    x=tf.keras.layers.ZeroPadding2D(((0, 0), (0, 96-input_width)))(img_input)\n",
        "    x = conv_block(x, 32, 3, 2, 2)\n",
        "    f1 = x\n",
        "    \n",
        "    # Block 2\n",
        "    x = conv_block(x, 64, 3, 2, 2)\n",
        "    f2 = x\n",
        "\n",
        "    # Block 3\n",
        "    x = conv_block(x, 128, 3, 2, 2)\n",
        "    f3 = x\n",
        "\n",
        "    # Block 4\n",
        "    x = conv_block(x, 256, 3, 2, 2)\n",
        "    f4 = x\n",
        "\n",
        "    #Block 5\n",
        "    x = conv_block(x, 256, 3, 2, 2)\n",
        "    f5 = x\n",
        "\n",
        "  \n",
        "    return (f1, f2, f3, f4, f5), img_input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbjYEQU8Eq-T"
      },
      "source": [
        "###Define FCN 8 decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giYEct_Se5Xj"
      },
      "source": [
        "'''\n",
        "This function defines the upsampling path taking the outputs of convolutions at each stage as arguments.\n",
        "Note the FCN - 8 style upsampling done in this function.\n",
        "'''\n",
        "def fcn8_decoder(convs, n_classes):\n",
        "  f1, f2, f3, f4, f5 = convs\n",
        "  n = 512\n",
        "  #Add convolutional layers on top of the CNN extractor.\n",
        "  o = ( tf.keras.layers.Conv2D( n , (7 , 7) , activation='relu' , padding='same', name=\"conv6\", data_format=IMAGE_ORDERING))(f5)\n",
        "  o = tf.keras.layers.Dropout(0.5)(o)\n",
        "\n",
        "  o = ( tf.keras.layers.Conv2D( n , (1 , 1) , activation='relu' , padding='same', name=\"conv7\", data_format=IMAGE_ORDERING))(o)\n",
        "  o = tf.keras.layers.Dropout(0.5)(o)\n",
        "   \n",
        "  ##Create a label map from the output of the CNN\n",
        "  o = tf.keras.layers.Conv2D(n_classes,  (1, 1), activation='relu' , padding='same', data_format=IMAGE_ORDERING)(o)\n",
        "  o = tf.keras.layers.Conv2DTranspose(n_classes , kernel_size=(4,4) ,  strides=(2,2) , use_bias=False, data_format=IMAGE_ORDERING )(o)\n",
        "  o = tf.keras.layers.Cropping2D(cropping=(1,1), data_format=IMAGE_ORDERING)(o)\n",
        "\n",
        "  o2 = f4\n",
        "  o2 = ( tf.keras.layers.Conv2D(n_classes , ( 1 , 1 ) , activation='relu' , padding='same', data_format=IMAGE_ORDERING))(o2)\n",
        "\n",
        "  o = tf.keras.layers.Add()([o, o2])\n",
        "\n",
        "  o = (tf.keras.layers.Conv2DTranspose( n_classes , kernel_size=(4,4) ,  strides=(2,2) , use_bias=False, data_format=IMAGE_ORDERING ))(o)\n",
        "\n",
        "  o2 = ( tf.keras.layers.Conv2D(n_classes , ( 1 , 1 ) , activation='relu' , padding='same', data_format=IMAGE_ORDERING))(f3)\n",
        "\n",
        "  o = tf.keras.layers.Cropping2D(cropping=(1, 1), data_format=IMAGE_ORDERING)(o)\n",
        "  o = tf.keras.layers.Add()([o, o2])\n",
        "     \n",
        "  o = tf.keras.layers.Conv2DTranspose(n_classes , kernel_size=(8,8) ,  strides=(8,8) , use_bias=False , data_format=IMAGE_ORDERING )(o)\n",
        "  o=tf.keras.layers.Cropping2D(((0, 0), (0, 96-84)))(o)\n",
        "\n",
        "  o = (tf.keras.layers.Activation('sigmoid'))(o)\n",
        "\n",
        "\n",
        "  return o"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EJEf484312h"
      },
      "source": [
        "img_input = tf.keras.layers.Input(shape=(64,84, 1))\n",
        "convs, img_input = FCN8(n_classes, img_input)\n",
        "dec_op = fcn8_decoder(convs, n_classes)\n",
        "model = tf.keras.Model(inputs = img_input, outputs = dec_op)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GAenp1M4gXx"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAAXygZtbZmu"
      },
      "source": [
        "##Compile Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpWpp8h4g_rE"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "510v0aVDXv1f"
      },
      "source": [
        "##Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HoZwpGWhMB-"
      },
      "source": [
        "EPOCHS = 70\n",
        "steps_per_epoch = 4000//BATCH_SIZE\n",
        "validation_steps = 800//BATCH_SIZE\n",
        "test_steps = 200//BATCH_SIZE\n",
        "\n",
        "\n",
        "history = model.fit(training_dataset,\n",
        "                    steps_per_epoch=steps_per_epoch, validation_data=validation_dataset, validation_steps=validation_steps, epochs=EPOCHS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eih-Q7GoXzJe"
      },
      "source": [
        "##Evaluate Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bTkaFM2X1gr"
      },
      "source": [
        "###Make Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zENjQuK0luH5"
      },
      "source": [
        "results = model.predict(test_Dataset, steps=test_steps)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_Uj_uuV9TQt"
      },
      "source": [
        "results = np.argmax(results, axis=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpKDUuAWX5Pj"
      },
      "source": [
        "###Visualize Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKTpLmly_RXb"
      },
      "source": [
        "def class_wise_metrics(y_true, y_pred):\n",
        "  class_wise_iou = []\n",
        "  class_wise_dice_score = []\n",
        "\n",
        "  smoothening_factor = 0.00001\n",
        "\n",
        "  for i in range(n_classes):\n",
        "    intersection = np.sum((y_pred == i) * (y_true == i))\n",
        "    y_true_area = np.sum((y_true == i))\n",
        "    y_pred_area = np.sum((y_pred == i))\n",
        "    combined_area = y_true_area + y_pred_area\n",
        "    \n",
        "    iou = (intersection) / (combined_area - intersection + smoothening_factor)\n",
        "    class_wise_iou.append(iou)\n",
        "    \n",
        "    dice_score =  2 * ((intersection) / (combined_area + smoothening_factor))\n",
        "    class_wise_dice_score.append(dice_score)\n",
        "\n",
        "  return class_wise_iou, class_wise_dice_score\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hkbsk_P1fpRM",
        "cellView": "both"
      },
      "source": [
        "#@title Visualize Output [RUN ME]\n",
        "integer_slider = 173 #@param {type:\"slider\", min:0, max:191, step:1}\n",
        "\n",
        "ds = test_Dataset.unbatch()\n",
        "ds = ds.batch(200)\n",
        "images = []\n",
        "\n",
        "y_true_segments = []\n",
        "for image, annotation in ds.take(1):\n",
        "  y_true_segments = annotation\n",
        "  images = image\n",
        "  \n",
        "  \n",
        "  \n",
        "iou, dice_score = class_wise_metrics(np.argmax(y_true_segments[integer_slider], axis=2), results[integer_slider])  \n",
        "show_annotation_and_prediction(image[integer_slider], annotation[integer_slider], results[integer_slider], iou, dice_score)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiG9K4t6X9iZ"
      },
      "source": [
        "###Compute IOU Score and Dice Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2706boF0CNNS"
      },
      "source": [
        "cls_wise_iou, cls_wise_dice_score = class_wise_metrics(np.argmax(y_true_segments, axis=3), results)\n",
        "\n",
        "\n",
        "for idx, (iou, dice_score) in enumerate(zip(cls_wise_iou[:-1], cls_wise_dice_score[:-1])):\n",
        "  print(\"Digit {}: IOU: {} Dice Score: {}\".format(idx, iou, dice_score)) \n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}